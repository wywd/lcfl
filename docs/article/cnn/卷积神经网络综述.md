# 卷积神经网络发展综述

## 0. 序言

CNN主要的经典结构包括：LeNet、AlexNet、ZFNet、VGG、NIN、GoogleNet、ResNet、SENet等，其发展过程如下图所示：  
![CNN发展历程](../pic/cnn/cnn历史.svg ':size=112%')  

## 1. LeNet（1998）

LeNet是卷积神经网络的开山鼻祖，第一次定义了CNN的网络结构。  

![LeNet网络结构](https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.5_lenet.png)  
LeNet网络分为卷积层和全连接层这两个部分，  
其中卷积模块包含三个部分：卷积、池化和非线性激活函数  

### LeNet主要特点：  
1. 定义了卷积神经网络的基本框架：输入层+卷积层+激活函数+池化层+全连接层  
2. 定义了卷积层，用于提取图像中的各种纹理特征，并具有全连接层没有的优势：局部连接（引入感受野）和权值共享（减少参数数量）  
3. 使用最大池化层，降低卷积层对位置的敏感性，同时可以减少参数数量，减轻过拟合  
4. 使用Tanh作为激活函数，相较于Sigmoid，Tanh以原点对称，收敛速度会更快。后来改进版本使用ReLU代替Tanh  

## 2. AlexNet （2012）

AlexNet于2012年提出，并在当年ImageNet竞赛中获得冠军，掀起深度学习热潮。其网络结构如图：  

![AlexNet网络结构](https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.6_alexnet.png)  

### AlexNet主要特点：
1. 使用ReLU作为激活函数，ReLU为非饱和函数，可以一定程度上缓解梯度消失的问题，加速模型收敛  
2. 引入Dropout操作避免模型过拟合。类似于浅层学习中的集成算法。  
3. 使用重叠的最大池化，避免了平均池化的模糊效果，同时因为池化的步长小于核尺寸，这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。  
4. 提出局部归一化(LRN)层，对局部神经元的活动创建竞争机制，使得响应较大的值变得相对更  大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。（不过效果一般，后面的论文几乎不再使用该操作）  
5. 采用双GPU网络结构  
6. 进行数据增强，对原始图像进行随机剪裁和随机镜像翻转，减轻过拟合，提高泛化能力  
7. 对训练图像做 PCA（主成分分析），利用服从 (0,0.1) 的高斯分布的随机变量对主成分进行扰动  

## 3. VGG （2014.09）

VGG系列模型包括VGG-11/VGG-13/VGG-16/VGG-19，它改进的方向是加深网络结构。
网络结构如图：  

![VGG网络结构](../pic/cnn/vgg.png ':size=80%')  

其网络参数数量统计如图：  

![VGG参数数量统计](../pic/cnn/vgg_para.png ':size=75%')  

### VGG网络结构细节：  
1. VGG 其实跟 AlexNet 有一定的相似之处，都是由五个卷积层与激活函数叠加的部分和三个全连接层组成，但是不同的是，VGG是5个卷积层组（Conv Layer Group），每个组内有2~3个卷积层，组与组之间连接一个最大池化层  
2. 每个卷积层组中feature_map的数量保持不变，越往后的卷积组feature_map数量越多，依次为：64-128-256-512-512
3. C组的VGG-16使用了1×1卷积（最早于Network in Network中提出），但是输入通道数和输出通道数没变，即没有发生降维，意义在于线性变换  
4. VGG参数数量主要消耗在最后3层全连接层，前面卷积层层数多但参数量不大，不过卷积层计算量大，训练更耗时

### VGG特点：  
1. 使用感受野小的3×3卷积核代替感受野大的卷积核（如5×5、7×7），同时VGG每个卷积层组包含2~3个3×3卷积核堆叠。2个3×3的卷积层串联相当于1个5×5的卷积层，即一个像素会跟周围5×5的像素产生关联，可以说感受野大小为5×5，而3个3×3的卷积层相当于1个7×7的卷积层，并且3个3×3的卷积层参数量为27，远小于1个7×7的卷积层参数量（为49）。因此，使用小卷积可以大幅度减少网络参数，同时增加了网络的深度，增加了网络的非线性表达能力（3个3×3卷积层使用3次ReLU函数，而1个7×7的卷积层只有一个ReLU）


## 4. NIN（2014）

之前的LeNet、AlexNet和VGG设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深。论文Network in Network提出了一个新的思路：即串联多个由卷积层和“全连接层”构成的小网络来构建一个深层的网络  

![NIN网络结构](../pic/cnn/nin.png ':size=100%')

### NIN网络特点：  
1. 提出了MLP-conv层，是一种微型网络结构，事实上，CNN高层特征其实是底层特征通过某种运算的组合，通过利用多层MLP的微型网络，可以对局部视野下的神经元进行更加复杂的运算，提高非线性。NIN使用2个1×1卷积核级联实现多通道feature_map的非线性组合，每个NiN模块中包含1个卷积层和2个1×1卷积层，它们之间使用ReLU作为激活函数，不同NiN模块之间进行最大池化操作。
2. 去掉最后3个全连接层，取而代之，使用全局均值池化层，即对每个特征图一整张图片进行全局均值池化，则每张特征图得到一个输出。这样采用均值池化，去除了构建全连接层的大量参数，大大减小网络规模，有效避免过拟合。


## 5. GoogLeNet（2014.09）

GoogLeNet又称为Inception V1，  其网络结构如图：  

![InceptionV1结构](../pic/cnn/Inception-v1.png ':size=150%')

![GoogLeNet](../pic/cnn/googLeNet.png  ':size=90%')

### GoogLeNet特点

1. 引入Inception模块，这只一种Network in Network结构，通过增加网络宽度，提高网络的表达能力，它可以替代人工确定卷积层中过滤器的类型或者是否创建卷积层和池化层等，让网络自己学习需要什么参数，即尝试找到卷积神经网络中最优局部稀疏结构。（Inception网络结构如上图）最终的Inception模块确定为右侧，通过1×1卷积对3×3和5×5卷积层进行降维，减少计算量，（这符合GoogLeNet不是为了通过增加模型大小来提高性能而是通过设计更巧妙的结构来提高性能，并   且尽可能降低模型复杂度的原则），同时低维映射同样保留了大量的图像信息。
2. 引入中间层的辅助loss单元。GoogLeNet3个loss单元，设置中间层额外的2个辅助loss，用以增加向后传导的梯度，缓解梯度消失问题，同时增加额外的正则化操作。
3. 使用全局均值池化代替后面的全连接层，使网络参数变得更少，因为GoogLeNet有着足够的深度和宽度，移除全连接层并没有影响结果的精度，同时使网络计算更快


## 6. InceptionV2（2015.02）


## 7. InceptionV3 （2015.12）


## 8. ResNet（2015.12）

## 9. InceptionV4（2016.02）

## 10. DenseNet（2016.08）

## 参考文献
[1] [卷积神经网络发展历程 CSDN](https://blog.csdn.net/u012679707/article/details/80870625)  
[2] [[总结] 卷积神经网络发展历程 知乎](https://zhuanlan.zhihu.com/p/76275427)  
[3] []()  




